{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple notebook to load the saved model and test dataset, then compute metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/processed/test.csv\")\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"../model_output/multilingual_model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "def tokenize_fn(example):\n",
    "    return tokenizer(\n",
    "        example['full_text'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.rename_column(\"category_label\", \"labels\")\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "precision_metric = load_metric(\"precision\")\n",
    "recall_metric = load_metric(\"recall\")\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for item in test_dataset:\n",
    "    input_ids = item[\"input_ids\"].unsqueeze(0)\n",
    "    attention_mask = item[\"attention_mask\"].unsqueeze(0)\n",
    "    labels = item[\"labels\"].unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(labels.item())\n",
    "\n",
    "acc = accuracy_metric.compute(predictions=all_preds, references=all_labels)[\"accuracy\"]\n",
    "prec = precision_metric.compute(predictions=all_preds, references=all_labels, average=\"weighted\")[\"precision\"]\n",
    "rec = recall_metric.compute(predictions=all_preds, references=all_labels, average=\"weighted\")[\"recall\"]\n",
    "f1 = f1_metric.compute(predictions=all_preds, references=all_labels, average=\"weighted\")[\"f1\"]\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A confusion matrix (using scikit-learn) to see how each category is predicted."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
